# -*- coding: utf-8 -*-
"""Logo_Classification_fk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bAjAXhhySihIgDBk8Gqypxn1Xl3W1Kqi
"""

!pip install tensorflow

import os
import numpy as np
import pandas as pd
from PIL import ImageOps
import matplotlib.pyplot as plt
from PIL import Image, ImageFilter
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation
from tensorflow.keras.preprocessing.image import DirectoryIterator, ImageDataGenerator
from tensorflow.keras.applications import Xception
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.callbacks import TensorBoard
import tensorflow as tf
from tensorflow.python.keras import Model
from tensorflow.python.keras.layers import Dense, Activation
from tensorflow.python.keras.utils.vis_utils import plot_model
from tensorflow.python.keras.callbacks import TensorBoard
from tensorflow import keras
from tensorflow.python.keras import Model
from tensorflow.python.keras.layers import Dense, Activation
from tensorflow.python.keras.utils.vis_utils import plot_model
from tensorflow.python.keras.callbacks import TensorBoard

train_img_labels_file = os.path.join(os.getcwd(),
                                    "flickr_logos_27_dataset_training_set_annotation.txt")

train_images = pd.read_csv(train_img_labels_file,
                           sep=' ',
                           usecols = [i for i in range(7)],
                           header=None,
                           names=["file_name",'label','subset','x1','y1','x2','y2'])
train_images.head()

test_img_label_file = os.path.join(os.getcwd(),
                                    "flickr_logos_27_dataset_query_set_annotation.txt")

test_images = pd.read_csv(test_img_label_file,
                           sep='\t',
                            usecols = [i for i in range(2)],
                           header=None,
                           names=["file_name",'label'])
test_images.head()

# Training images

plt.figure(figsize=(16, 10))
for idx in range(16):
    ax = plt.subplot(4, 4, idx + 1)
    plt.imshow(Image.open('data/'+ train_images.loc[idx,'file_name']))
    plt.title(train_images.loc[idx,'label'])
    plt.axis("off")

# Same for test images
plt.figure(figsize=(16, 10))
for idx in range(16):
    ax = plt.subplot(4, 4, idx + 1)
    plt.imshow(Image.open('data/'+ test_images.loc[idx,'file_name']))
    plt.title(test_images.loc[idx,'label'])
    plt.axis("off")

"""### Data Preprocessing ###"""

#check labels

train_labels = train_images["label"].astype(str).unique()

train_labels

"""### Creating dataset and splitting into training and validation sets ###"""

# crop the images as per the annotations file and save a new set of training image instances to a sub-folder
data_dir = 'data'
# Multi-class categorical labels
train_labels = train_images["label"].astype("str")

# Load and preprocess images
train_images_list = []
for _, row in train_images.iterrows():
    img = tf.keras.preprocessing.image.load_img(os.path.join(data_dir, row["file_name"]), target_size=(128, 128))
    img = tf.keras.preprocessing.image.img_to_array(img)
    img = img / 255.0  # Normalize to [0, 1]
    train_images_list.append(img)

train_images_array = np.array(train_images_list)

# One-hot encode the labels for multi-class classification
train_labels_encoded = pd.get_dummies(train_labels)

# Data preprocessing for the test set
# Multi-class categorical labels
test_labels = test_images["label"].astype("str")

# Load and preprocess test images
test_images_list = []
for _, row in test_images.iterrows():
    img = tf.keras.preprocessing.image.load_img(os.path.join(data_dir, row["file_name"]), target_size=(128, 128))
    img = tf.keras.preprocessing.image.img_to_array(img)
    img = img / 255.0  # Normalize to [0, 1]
    test_images_list.append(img)

test_images_array = np.array(test_images_list)

# One-hot encode the test labels
test_labels_encoded = pd.get_dummies(test_labels)

# Merge training data into a DataFrame
train_data = pd.DataFrame({"image": train_images_list, "label": train_labels})

# Filter samples where labels are the same in both training and test sets
common_labels = set(train_data["label"].unique()) & set(test_labels.unique())

filtered_train_data = train_data[train_data["label"].isin(common_labels)]

# Data preprocessing for the test set
# Filter test labels based on common labels
filtered_test_labels = test_labels[test_labels.isin(common_labels)]

# Filter test images based on common labels
filtered_test_images_array = test_images_array[test_labels.isin(common_labels)]

# One-hot encode the test labels
test_labels_encoded = pd.get_dummies(filtered_test_labels)

test_labels_encoded

"""#### CNN model using Xception as a base ####"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import Xception
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Build a CNN model using Xception as a base
base_model = Xception(weights='imagenet', include_top=False)
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.6)(x)
# Adjust the number of output units to match the number of classes in your dataset (27 classes)
num_output_units = 27
predictions = Dense(num_output_units, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

# Freeze base model layers
for layer in base_model.layers:
    layer.trainable = False

# Compile the model
model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(train_images_array, train_labels_encoded, epochs=10, batch_size=32, validation_data=(filtered_test_images_array, test_labels_encoded))

# Evaluate the model
test_loss, test_accuracy = model.evaluate(filtered_test_images_array, test_labels_encoded, verbose=2)
print(f'Test accuracy: {test_accuracy:.2%}')

"""###### Plotting performance graphs ###


"""

# Visualize the training history
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

"""### Building a Convolutional Neural Network (CNN) for image classification ###"""

from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Build a Convolutional Neural Network (CNN)
model = tf.keras.Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
    MaxPooling2D(2, 2),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.6),
    Dense(num_output_units, activation='softmax')
])

model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

#Train the model using the training data.
history = model.fit(train_images_array, train_labels_encoded, epochs=10, batch_size=32, validation_data=(filtered_test_images_array, test_labels_encoded))


#Evaluate the Model:
test_loss, test_accuracy = model.evaluate(filtered_test_images_array, test_labels_encoded, verbose=2)
print(f'Test accuracy: {test_accuracy:.2%}')

"""### Plotting performance graphs ###"""

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()